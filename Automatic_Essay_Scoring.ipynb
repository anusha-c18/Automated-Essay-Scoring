{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1xbMxpgYWI-8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbMiT7KDDsGD"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ygtz_X3TYR8v"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QAjSBysZOCM"
      },
      "outputs": [],
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUGEu7JVZQUR"
      },
      "outputs": [],
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgfZFP9dYIa3"
      },
      "outputs": [],
      "source": [
        "! kaggle competitions download -c asap-aes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6loj9gWcjmZ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/training_set_rel3.tsv.zip\", sep='\\t', encoding='ISO-8859-1');\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odrrXSHiTqQY"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4t0hzWWdPqx"
      },
      "outputs": [],
      "source": [
        "df.dropna(axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9QFZrhxdR1U"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HA8QcDJdX7K"
      },
      "outputs": [],
      "source": [
        "df.drop(columns=['rater1_domain1','rater2_domain1'],inplace=True,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FE0VHCVLCcO-"
      },
      "outputs": [],
      "source": [
        "#Check Skewness\n",
        "sns.distplot(df['domain1_score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBOx2oM0EeGX"
      },
      "outputs": [],
      "source": [
        "min_range = [2,1,0,0,0,0,0,0]\n",
        "max_range = [12,6,3,3,4,4,30,60]\n",
        "\n",
        "def normalize(x,mi,ma):\n",
        "    #print(\"Before Normalization: \"+str(x))\n",
        "    x = (x-mi)/(ma-mi)\n",
        "    #print(\"After Normalization : \"+str(x))\n",
        "    return round(x*10)\n",
        "\n",
        "df['final_score']=df.apply(lambda x:normalize(x['domain1_score'],min_range[x['essay_set']-1],max_range[x['essay_set']-1]),axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oz1diuveebF"
      },
      "outputs": [],
      "source": [
        "sns.distplot(df['final_score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84_5YTr8elzl"
      },
      "outputs": [],
      "source": [
        "df.drop('domain1_score',axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omwyChSkelra"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emoLhAUjelbs"
      },
      "outputs": [],
      "source": [
        "def clean_essay(essay):\n",
        "    x=[]\n",
        "    for i in essay.split():\n",
        "        if i.startswith(\"@\"):\n",
        "            continue\n",
        "        else:\n",
        "            x.append(i)\n",
        "    return ' '.join(x)\n",
        "\n",
        "df['essay'] = df['essay'].apply(lambda x:clean_essay(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfgvGDWLe-uo"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english')) \n",
        "def remove_stop_words(essay):\n",
        "    word_tokens = word_tokenize(essay) \n",
        "    filtered_sentence = [] \n",
        "    for w in word_tokens: \n",
        "        if w not in stop_words: \n",
        "            filtered_sentence.append(w)\n",
        "    return ' '.join(filtered_sentence)\n",
        "\n",
        "df['clean_essay'] = df['essay'].apply(lambda x:remove_stop_words(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6nzG5EvfN53"
      },
      "outputs": [],
      "source": [
        "def remove_puncs(essay):\n",
        "    essay = re.sub(\"[^A-Za-z ]\",\"\",essay)\n",
        "    return essay\n",
        "\n",
        "df['clean_essay'] = df['clean_essay'].apply(lambda x:remove_puncs(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBbUMH3XfO0H"
      },
      "outputs": [],
      "source": [
        "def sent2word(x):\n",
        "    global words\n",
        "    x=re.sub(\"[^A-Za-z0-9]\",\" \",x)\n",
        "    words=nltk.word_tokenize(x)\n",
        "    return words\n",
        "\n",
        "def essay2word(essay):\n",
        "    essay = essay.strip()\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    raw = tokenizer.tokenize(essay)\n",
        "    final_words=[]\n",
        "    for i in raw:\n",
        "        if(len(i)>0):\n",
        "            final_words.append(sent2word(i))\n",
        "    return final_words\n",
        "        \n",
        "\n",
        "def noOfWords(essay):\n",
        "    count=0\n",
        "    for i in essay2word(essay):\n",
        "        count=count+len(i)\n",
        "    return count\n",
        "\n",
        "def noOfChar(essay):\n",
        "    count=0\n",
        "    for i in essay2word(essay):\n",
        "        for j in i:\n",
        "            count=count+len(j)\n",
        "    return count\n",
        "\n",
        "def avg_word_len(essay):\n",
        "    return noOfChar(essay)/noOfWords(essay)\n",
        "\n",
        "def noOfSent(essay):\n",
        "    return len(essay2word(essay))\n",
        "\n",
        "def count_pos(essay):\n",
        "    sentences = essay2word(essay)\n",
        "    noun_count=0\n",
        "    adj_count=0\n",
        "    verb_count=0\n",
        "    adverb_count=0\n",
        "    for i in sentences:\n",
        "        pos_sentence = nltk.pos_tag(i)\n",
        "        for j in pos_sentence:\n",
        "            pos_tag = j[1]\n",
        "            if(pos_tag[0]=='N'):\n",
        "                noun_count+=1\n",
        "            elif(pos_tag[0]=='V'):\n",
        "                verb_count+=1\n",
        "            elif(pos_tag[0]=='J'):\n",
        "                adj_count+=1\n",
        "            elif(pos_tag[0]=='R'):\n",
        "                adverb_count+=1\n",
        "    return noun_count,verb_count,adj_count,adverb_count\n",
        "\n",
        "#data = open('big.txt').read()\n",
        "#words = re.findall('[a-z]+', data.lower())\n",
        "\n",
        "def check_spell_error(essay):\n",
        "    essay=essay.lower()\n",
        "    new_essay = re.sub(\"[^A-Za-z0-9]\",\" \",essay)\n",
        "    new_essay = re.sub(\"[0-9]\",\"\",new_essay)\n",
        "    count=0\n",
        "    all_words = new_essay.split()\n",
        "    for i in all_words:\n",
        "        if i not in words:\n",
        "            count+=1\n",
        "    return count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4h1H-hCgZ_b"
      },
      "outputs": [],
      "source": [
        "#prep for ML\n",
        "vectorizer = CountVectorizer(max_features = 10000, ngram_range=(1, 3), stop_words='english')\n",
        "count_vectors = vectorizer.fit_transform(df['clean_essay'])\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "data = df[['essay_set','clean_essay','final_score']].copy()\n",
        "X = count_vectors.toarray()\n",
        "y = data['final_score'].to_numpy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGRobO5ygd77"
      },
      "outputs": [],
      "source": [
        "#ml with pre processing\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "pro_data = df.copy()\n",
        "pro_data['char_count'] = pro_data['essay'].apply(noOfChar)\n",
        "pro_data['word_count'] = pro_data['essay'].apply(noOfWords)\n",
        "pro_data['sent_count'] = pro_data['essay'].apply(noOfSent)\n",
        "pro_data['avg_word_len'] = pro_data['essay'].apply(avg_word_len)\n",
        "pro_data['spell_err_count'] = pro_data['essay'].apply(check_spell_error)\n",
        "pro_data['noun_count'], pro_data['adj_count'], pro_data['verb_count'], pro_data['adv_count'] = zip(*pro_data['essay'].map(count_pos))\n",
        "pro_data.to_csv(\"Processed_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RuZo0Dgjlfh"
      },
      "outputs": [],
      "source": [
        "prep_df = pd.read_csv(\"Processed_data.csv\")\n",
        "prep_df.drop('Unnamed: 0',inplace=True,axis=1)\n",
        "prep_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1uSs-Hzfju0B",
        "outputId": "4025ca07-ebc1-4dac-a220-64ba483bc239"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-0c1efdbf5d6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Saved_Models/RF_with_PP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#Use Saved Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Saved_Models/RF_with_PP'"
          ]
        }
      ],
      "source": [
        "#Random forest\n",
        "#Save Trained Model\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
        "rf.fit(X_train, y_train)\n",
        "pickle.dump(rf, open('Saved_Models/RF_with_PP', 'wb'))\n",
        "\n",
        "#Use Saved Model\n",
        "rf = pickle.load(open('Saved_Models/RF_wth_pp', 'rb'))\n",
        "y_pred = rf.predict(X_test)\n",
        "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
        "from keras.models import Sequential, load_model, model_from_config\n",
        "import keras.backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import cohen_kappa_score"
      ],
      "metadata": {
        "id": "_uCMHKfIPYxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7QDfEgCMkz0K"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"Dataset/training_set_rel3.tsv\", sep='\\t', encoding='ISO-8859-1');\n",
        "df.dropna(axis=1,inplace=True)\n",
        "df.drop(columns=['domain1_score','rater1_domain1','rater2_domain1'],inplace=True,axis=1)\n",
        "df.head()\n",
        "temp = pd.read_csv(\"Processed_data.csv\")\n",
        "temp.drop(\"Unnamed: 0\",inplace=True,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['domain1_score']=temp['final_score']\n",
        "df.head()"
      ],
      "metadata": {
        "id": "dP9P8XrXRHz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['essay'][0]"
      ],
      "metadata": {
        "id": "bWsXBWVcRMaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp.head(1)"
      ],
      "metadata": {
        "id": "-HL2X5Z6RRA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make Dataset\n",
        "y = df['domain1_score']\n",
        "df.drop('domain1_score',inplace=True,axis=1)\n",
        "X=df"
      ],
      "metadata": {
        "id": "OqRnEAHNRWur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "NnCPk2BERp_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "RvESeUjfRsM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_e = X_train['essay'].tolist()\n",
        "test_e = X_test['essay'].tolist()"
      ],
      "metadata": {
        "id": "9jXHe7f3Rwcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sents=[]\n",
        "test_sents=[]\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "def sent2word(x):\n",
        "    x=re.sub(\"[^A-Za-z]\",\" \",x)\n",
        "    x.lower()\n",
        "    filtered_sentence = [] \n",
        "    words=x.split()\n",
        "    for w in words:\n",
        "        if w not in stop_words: \n",
        "            filtered_sentence.append(w)\n",
        "    return filtered_sentence\n",
        "\n",
        "def essay2word(essay):\n",
        "    essay = essay.strip()\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    raw = tokenizer.tokenize(essay)\n",
        "    final_words=[]\n",
        "    for i in raw:\n",
        "        if(len(i)>0):\n",
        "            final_words.append(sent2word(i))\n",
        "    return final_words\n",
        "\n",
        "for i in train_e:\n",
        "    train_sents+=essay2word(i)\n",
        "\n",
        "for i in test_e:\n",
        "    test_sents+=essay2word(i)"
      ],
      "metadata": {
        "id": "yMJHqJr0R1yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_sents)"
      ],
      "metadata": {
        "id": "5oNX3hmXR-YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sents[0]"
      ],
      "metadata": {
        "id": "ysnDuipzSD_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n",
        "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='relu'))\n",
        "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "metadata": {
        "id": "5o-9nsv-SLGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Word2Vec model\n",
        "num_features = 300 \n",
        "min_word_count = 40\n",
        "num_workers = 4\n",
        "context = 10\n",
        "downsampling = 1e-3\n",
        "\n",
        "model = Word2Vec(train_sents, \n",
        "                 workers=num_workers, \n",
        "                 size=num_features, \n",
        "                 min_count = min_word_count, \n",
        "                 window = context, \n",
        "                 sample = downsampling)\n",
        "\n",
        "model.init_sims(replace=True)\n",
        "model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)"
      ],
      "metadata": {
        "id": "fF5pzrynSR-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def makeVec(words, model, num_features):\n",
        "    vec = np.zeros((num_features,),dtype=\"float32\")\n",
        "    noOfWords = 0.\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    for i in words:\n",
        "        if i in index2word_set:\n",
        "            noOfWords += 1\n",
        "            vec = np.add(vec,model[i])        \n",
        "    vec = np.divide(vec,noOfWords)\n",
        "    return vec\n",
        "\n",
        "\n",
        "def getVecs(essays, model, num_features):\n",
        "    c=0\n",
        "    essay_vecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
        "    for i in essays:\n",
        "        essay_vecs[c] = makeVec(i, model, num_features)\n",
        "        c+=1\n",
        "    return essay_vecs\n",
        "\n",
        "\n",
        "clean_train=[]\n",
        "for i in train_e:\n",
        "    clean_train.append(sent2word(i))\n",
        "training_vectors = getVecs(clean_train, model, num_features)\n",
        "\n",
        "clean_test=[] \n",
        "\n",
        "for i in test_e:\n",
        "    clean_test.append(sent2word(i))\n",
        "testing_vectors = getVecs(clean_test, model, num_features)"
      ],
      "metadata": {
        "id": "hAxlAD7XSWUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_vectors.shape"
      ],
      "metadata": {
        "id": "1cqlr1mrSdim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_vectors = np.array(training_vectors)\n",
        "testing_vectors = np.array(testing_vectors)\n",
        "\n",
        "# Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n",
        "training_vectors = np.reshape(training_vectors, (training_vectors.shape[0], 1, training_vectors.shape[1]))\n",
        "testing_vectors = np.reshape(testing_vectors, (testing_vectors.shape[0], 1, testing_vectors.shape[1]))\n",
        "lstm_model = get_model()"
      ],
      "metadata": {
        "id": "12Gehok9Sh1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_vectors.shape"
      ],
      "metadata": {
        "id": "mkRX8vwaS6ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model.fit(training_vectors, y_train, batch_size=64, epochs=150)"
      ],
      "metadata": {
        "id": "7qhI8eJuS67B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model.save('final_lstm.h5')\n",
        "y_pred = lstm_model.predict(testing_vectors)\n",
        "y_pred = np.around(y_pred)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "y_Zn_TJgTVw-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Automatic_Essay_Scoring.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}